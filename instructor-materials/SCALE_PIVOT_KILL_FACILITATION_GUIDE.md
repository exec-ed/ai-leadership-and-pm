# SCALE/PIVOT/KILL EXERCISE
## Detailed Facilitation Guide

**Activity Duration:** 45 minutes total (30 min work + 3 min presentations + 10 min debrief)
**Difficulty:** Hard
**Key Skills Being Tested:** Data-driven decision-making, applying your own criteria, business judgment

---

## OVERVIEW

After the Crisis Simulation, groups use the **Go/No-Go criteria they defined in Pilot Scoping** to analyze pilot data and make a final decision: **Scale, Pivot, or Kill.**

This exercise forces the hard question: *"Based on the evidence, what's the right call?"*

---

## SETUP BEFORE THE EXERCISE

### What You Give Groups

1. **Their original Pilot Scoping worksheet** (from Exercise 1)
   - Their success metrics
   - Their Go/No-Go criteria
   - Their original assumptions

2. **Pilot performance data** (you create this based on their decisions and the crises they went through)
   - Metric dashboard (accuracy, satisfaction, response time, cost, adoption, etc.)
   - Qualitative feedback from customers, team, leadership
   - Incident log from the crises

3. **Decision template** (1 page)
   - Criteria met: Yes/No for each
   - Recommendation: Scale/Pivot/Kill
   - Rationale: 2-3 paragraphs

### Timing
- **30 minutes:** Groups analyze data and write recommendation
- **15 minutes:** Each group presents (2-3 min per group, depends on how many you have)
- **10 minutes:** Debrief and key learning points

---

## SAMPLE PILOT DATA TO USE

You can customize this based on your actual facilitation, but here's what good data looks like:

### KEY METRICS (End of 6-week pilot)

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| **Accuracy** | ≥90% | 88% | ⚠️ Close but not there |
| **Customer Satisfaction** | ≥80% | 82% | ✓ Met |
| **Response Time** | <4 hours | 3.5 hours | ✓ Met |
| **Cost per Query** | $12 (was $18) | $14 | ⚠️ Below target |
| **Team Adoption** | 80%+ | 75% | ⚠️ Close |
| **Escalations** | <2% | 3.5% | ❌ Above target |

### QUALITATIVE FEEDBACK

**From Customers:**
- 82% satisfied overall
- "Fast and accurate most of the time"
- "Sometimes wrong info, but could escalate to human"
- "Better than 26-hour email wait"

**From Team:**
- "AI handles the boring stuff, frees us for real work"
- "Some accuracy issues, but mostly good"
- "Still worried about future" (2 agents)
- "Would use more if accuracy improved"

**From Leadership:**
- CEO: "Good progress. Want to see accuracy at 90% before scaling."
- CFO: "Cost savings not as high as projected. Need to recalibrate."
- Data Scientist: "Can hit 92% accuracy in 4 more weeks with additional training."

### INCIDENT LOG
- Week 1: Data quality issue (fixed)
- Week 2: Team resistance (addressed)
- Week 3: Escalations spiked (normalized)
- Week 4-6: Steady performance

---

## THREE DECISION PATHS

### PATH 1: SCALE ❌ (Not Recommended with This Data)

**What it means:** Roll out the AI system to 100% of customer service across all stores.

**Why groups might choose it:**
- Satisfaction is high (82%)
- Response time is good (3.5 hours)
- Team adoption is decent (75%)
- "We spent $150K, we have to make it work"

**Why it's wrong (given the data):**
- Accuracy is 88% vs. target 90% (below target)
- Escalations are 3.5% vs. target <2% (above target)
- Cost is $14 vs. target $12 (above target)
- **You only meet 3 of 6 criteria**
- Scaling too early = scaling problems to production

**Staff feedback if groups choose this:**
> "You met 50% of your criteria. That's not 'ready to scale.' You'd be rolling out a system you yourself said needs work. Why scale before you've proven it works?"

---

### PATH 2: PIVOT ✓ (RECOMMENDED)

**What it means:** Continue the pilot with modifications. Extend 4 more weeks focusing on accuracy, then reassess.

**Why groups should choose it:**
- **Gaps are solvable:** Accuracy is close (2 points away), escalations normalized, cost is understandable at small scale
- **Proof of concept exists:** 82% satisfaction, 3.5-hour response time shows value
- **Team is engaged:** 75% adoption is solid for new technology
- **4-week investment is low-risk:** Data scientist can hit 92% accuracy in that time
- **You show leadership you're data-driven:** "Wanted to scale but data doesn't support it YET"

**What the pivot looks like:**
1. Spend 4 more weeks improving accuracy (model retraining, data refinement)
2. Optionally add Phase 2 scope (returns policy queries) to increase volume
3. Reassess at week 10 for scale decision
4. New decision point with better data

**Staff feedback if groups choose this:**
> "Exactly. You're not scaling because you don't have the evidence yet. But you're not killing it because you HAVE strong signals. You're extending to get clearer data. That's how leaders make decisions."

---

### PATH 3: KILL ❌ (Also Not Recommended with This Data)

**What it means:** Stop the project. Admit the AI approach isn't working. Return to 100% human customer service.

**Why groups might choose it:**
- "We didn't hit enough metrics"
- "Too many problems (data quality, team resistance, escalations)"
- "Better to kill now than fail at scale"

**Why it's wrong (given the data):**
- **You have proof of value:** 82% satisfaction, 3.5-hour response time
- **You have team adoption:** 75% is solid for new tech
- **You don't have evidence it fundamentally can't work:** The gaps are solvable
- **You're 2 percentage points away on accuracy:** That's not "doesn't work," that's "needs refinement"

**Staff feedback if groups choose this:**
> "You have enough signal to continue. You DON'T have signal that the system fundamentally fails. Kill is too extreme. You're confusing 'not perfect yet' with 'broken.'"

---

## FACILITATING THE WORK PHASE (30 minutes)

### Your Role During This Phase

**Don't just sit and watch.** Actively facilitate the thinking:

---

### At 5 Minutes In
> "Good, I see you analyzing the data. As you look at each metric, ask yourself: Did we hit our target or not? Keep score."

---

### At 15 Minutes In
> "You should be getting close to a decision. Ask yourselves: Are the gaps fundamental problems or just things that need more time?"

---

### At 25 Minutes In
> "10 minutes left. Write down your recommendation. What's your one-sentence decision and why?"

---

### If Groups Get Stuck

**Use these guiding questions:**

**Question 1: Scorecard**
> "Let's count. How many of your success criteria did you actually meet?"
>
> (They list them out)
>
> "Okay, so 3 out of 6. Does that feel like 'ready to scale'?"

---

**Question 2: Solvable vs. Fundamental**
> "Accuracy is 88 vs. 90 target. Is that a fundamental problem (the AI approach doesn't work) or a solvable problem (needs more training)?"
>
> (Usually solvable)
>
> "Okay. What about escalations at 3.5% vs. <2%? Fundamental or solvable?"
>
> (Usually solvable—they normalized after week 3)

---

**Question 3: Key Assumption**
> "What was your biggest assumption about this project when you started?"
>
> (e.g., "AI can work in customer service")
>
> "Did the pilot test that assumption?"
>
> "Did the assumption hold up?"
>
> (e.g., "Yes, satisfaction is 82%, so yes, it works")
>
> "Okay. So you've PROVEN the assumption. You just haven't proven you're READY to scale yet. Different questions."

---

**Question 4: Board Presentation**
> "Imagine you're presenting this to the board. You say: 'We want to scale the AI to 100% of customer service.'"
>
> "Board asks: 'Why?'"
>
> "You answer: [your reasoning]"
>
> "Does that sound convincing to you?"
>
> (Usually helps them see if their logic holds)

---

### Common Statements You'll Hear & How to Push Back

---

**Statement:** "We spent $150K, so we have to scale it."

**Respond:** "The $150K is sunk cost. That money is gone. The only question that matters is: Will scaling this create more value than other options? Justify it forward, not backward."

---

**Statement:** "It's almost working, so let's scale."

**Respond:** "88% is close to 90%, but it's not 90%. Would you fly on a plane with 88% safety rating? No. Same principle. 'Almost there' is exactly why you pivot, not scale."

---

**Statement:** "Customers are happy, so we should scale."

**Respond:** "Customers ARE happy (82%)—that's great. But you also have escalations at 3.5% vs. target <2%. Both metrics matter. You can't scale on one good metric if other metrics fail."

---

**Statement:** "We can scale and fix problems in production."

**Respond:** "Remember Crisis #1? That's what happens when you scale with known issues. You had the data quality disaster at 30% of queries. Now imagine that at 100%. You have 4 weeks to improve accuracy. Use it."

---

## FACILITATING PRESENTATIONS (15 minutes)

### How Groups Present

Each group presents:
- **Their recommendation:** Scale/Pivot/Kill (1 min)
- **Their reasoning:** Why this decision based on the data (2 min)

That's it. Keep it short.

---

### What YOU Should Listen For

**Good decision-making signs:**
- ✓ References their original criteria explicitly ("We said accuracy ≥90%")
- ✓ Analyzes ALL the data, not just the good parts
- ✓ Distinguishes between gaps (solvable) and failures (fundamental)
- ✓ Clear decision (not wishy-washy)
- ✓ Can explain trade-offs

**Poor decision-making signs:**
- ❌ Ignores their own criteria
- ❌ Cherry-picks only positive data
- ❌ Can't explain their reasoning
- ❌ "I think..." without data
- ❌ Defends decision by talking about money spent

---

### How to Question Groups

**After they present, ask:**

**Question 1: Criteria Check**
> "You recommended [Scale/Pivot/Kill]. Which of your original criteria supported that decision?"
>
> (If they can't reference them, they weren't data-driven)

---

**Question 2: The Gap**
> "You missed the accuracy target (88% vs. 90%). Why did that not kill your decision to scale?"
>
> (Tests whether they understand trade-offs)

---

**Question 3: Confidence Level**
> "On a scale of 1-10, how confident are you in this decision?"
>
> (9-10 = confident, clear decision)
> (5-7 = uncertain, decision wasn't clear)
> (1-4 = not confident, probably wrong decision)

---

**Question 4: Reversal**
> "If [X metric] was at target instead of where it is, would you change your decision?"
>
> (Tests if they understand which metrics actually matter)

---

**Question 5: The Debate**
> "I noticed you disagreed internally. What was the debate?"
>
> (Reveals the actual trade-off thinking)

---

## FACILITATING DEBRIEF (10 minutes)

### If Most Groups Chose the Same Decision

**Example: Most chose PIVOT**

> "I notice most of you chose Pivot. Why do you think that was the most common choice?"
>
> (Listen to their reasoning)
>
> "What would have to be true for you to choose Scale instead of Pivot?"
>
> (e.g., "If accuracy was at 90%")
>
> "And what would have to be true for you to choose Kill?"
>
> (e.g., "If satisfaction was below 60%")

This helps them see the decision framework clearly.

---

### If Groups Chose Different Decisions

**This is great.** Use it to explore disagreements:

> "You chose Scale, they chose Pivot. Let's debate. What data supports Scale that Pivot groups missed?"
>
> (Let them argue it out)
>
> "And Pivot group, what does your data show that Scale group overlooked?"
>
> (The debate reveals decision-making logic)

---

### Key Teaching Point to Land

> "Notice what you just did: You analyzed ambiguous data and made a call. You didn't have perfect information. You made the best decision you could with what you had.
>
> That's project management. In real projects, you rarely have perfect data. You make calls with 70% certainty. That's what leadership looks like."

---

---

## EVALUATION RUBRIC (If You Want to Score Decisions)

### 9-10: Strong Decision-Making
- ✓ Explicitly references original Go/No-Go criteria
- ✓ Analyzes ALL data points (doesn't cherry-pick)
- ✓ Identifies which gaps are solvable vs. fundamental
- ✓ Makes a clear recommendation with confidence
- ✓ Can defend decision against pushback
- ✓ Shows understanding of trade-offs

**Example:**
> "We miss accuracy (88% vs. 90%) and escalations (3.5% vs. <2%), but hit satisfaction (82%), response time, and adoption (75%). Data scientist says 4 weeks to accuracy. PIVOT: Extend 4 weeks, improve accuracy, reassess at week 10. This is data-driven and shows confidence."

---

### 7-8: Competent Decision-Making
- ~ Uses criteria but not explicitly referenced
- ~ Analyzes most (but not all) data points
- ~ Decision is clear but reasoning could be stronger
- ~ Shows understanding of some trade-offs
- ~ Can defend decision if asked

**Example:**
> "The metrics are mixed. Some good, some bad. We should Pivot and try to fix the accuracy. 4 more weeks seems reasonable."

---

### 5-6: Developing Decision-Making
- ~ References criteria vaguely
- ~ Analyzes only some data
- ~ Decision is unclear or conflicts with data
- ~ Limited trade-off thinking
- ~ Struggles to defend decision

**Example:**
> "We spent a lot of money. Customers are happy. We should probably Scale but maybe fix some things first."

---

### <5: Needs Development
- ❌ No reference to criteria
- ❌ Cherry-picks positive data only
- ❌ Decision doesn't follow from data
- ❌ No clear reasoning provided
- ❌ Can't defend decision

**Example:**
> "I think this is good. We should Scale it."

---

---

## CONNECTIONS TO EARLIER ACTIVITIES

**This exercise ties EVERYTHING together:**

### Pilot Scoping (Exercise 1)
- You defined: "Accuracy ≥90%"
- Now: "We're at 88%, so we don't scale YET"
- The criteria you set upfront made this decision easier

### Speed-Dating (Exercise 0)
- You understand stakeholder concerns:
  - CEO wants ROI (look at cost data)
  - Data Scientist wants accuracy (look at 88% vs. 90%)
  - Manager wants adoption (look at 75%)
- Now you balance them

### Pilot Scoping Debrief
- You discussed different team approaches
- Now you apply those lessons
- Data-driven scope leads to clearer decisions

### Crisis Simulation (Exercise 2)
- You learned that issues are often solvable
- Crisis #1: Data problem → Fixed with retraining (Pivot)
- Crisis #2: Team problem → Fixed with engagement (Pivot)
- Now: Use that mindset here

### This Exercise (Exercise 3)
- **Apply it all to a real decision**
- Data + Criteria + Stakeholder thinking + Leadership judgment = This decision

---

---

## VARIATIONS (If You Have Time/Preferences)

### Variation 1: Conflicting Data Scenarios

Create different versions of the data for different groups:

- **Group A: "Scale Scenario"** (very strong data, should scale)
- **Group B: "Pivot Scenario"** (mixed data, should pivot)
- **Group C: "Kill Scenario"** (weak data, should kill)

Then have them debate why the other scenarios are wrong.

**Why this works:** Tests whether they understand the criteria logic, not just the answer.

---

### Variation 2: Executive Pushback Role-Play

After a group presents their decision:

**You play CEO:**
> "You want to KILL the project? We spent $150K on this. Why would we walk away?"
>
> (OR if they chose Scale)
>
> "You want to SCALE when accuracy is at 88%? That's not good enough."

**Group has to defend their decision.**

**Why this works:** Tests whether they can defend data-driven decisions under pressure (real PM skill).

---

### Variation 3: Budget Constraint

Add a financial twist:
> "You can extend the pilot 4 more weeks (Pivot) OR you can pivot with half the budget. Choose one."

**Why this works:** Forces trade-off thinking. Sometimes you pivot on money constraints, not just data.

---

---

## COMMON FACILITATION MISTAKES TO AVOID

### ❌ Mistake 1: Revealing the "Right Answer" Too Early

**Don't say:** "The answer is Pivot."

**Instead:** Let groups debate and discover it through the data.

---

### ❌ Mistake 2: Not Pushing Back on Weak Reasoning

**If group says:** "We should scale because satisfaction is high."

**Don't let it slide.** Push back: "What about the metrics you DIDN'T hit?"

---

### ❌ Mistake 3: Rushing the Presentations

**Don't:** "Okay, who's next?" (cuts off good discussion)

**Do:** Ask follow-up questions. Let them defend their thinking.

---

### ❌ Mistake 4: Letting Sunk Cost Fallacy Win

**If group justifies by:** "We spent $150K..."

**Correct them:** "That's sunk cost thinking. Only future returns matter."

---

### ❌ Mistake 5: Accepting Wishy-Washy Decisions

**If group says:** "Maybe we should pivot... or scale... we're not sure."

**Push back:** "You have to choose. What's your call and why?"

---

---

## KEY LEARNING POINTS TO LAND

### 1. "Data Beats Gut Feel"
You have actual numbers. Use them. Don't say "I think we're ready" when the data says you're not.

### 2. "Go/No-Go Criteria Are Your Anchor"
If you defined criteria upfront (Pilot Scoping), this decision becomes much easier. "We said accuracy ≥90%. We're at 88%. We don't scale." Done.

### 3. "Pivot Is Often the Right Answer"
Not everything is Scale vs. Kill. Most projects are "Pivot: continue with changes." That's actually the most common decision in real projects.

### 4. "Sunk Costs Are Sunk"
The $150K is gone. Don't use it to justify future spending. Only forward returns matter.

### 5. "You Can Decide with Ambiguous Data"
Perfect data doesn't exist. You make the best call you can. That's the PM job.

### 6. "Decision Quality = Reasoning Quality"
Not whether you chose Scale/Pivot/Kill, but whether your reasoning was sound. A well-reasoned Pivot is better than a poorly-reasoned Scale.

---

---

## TIMING SUMMARY

| Time | Activity | Your Role |
|------|----------|-----------|
| 0-2 min | Setup: Explain the exercise | Give groups the data and template |
| 2-30 min | Work phase | Facilitate thinking, ask guiding questions |
| 30-45 min | Presentations | Listen for decision quality, ask follow-ups |
| 45-55 min | Debrief | Land the key learning points |

---

---

## FOR YOUR FACILITATION SESSION

### Before the Exercise
- ✓ Print pilot data sheets (or display on screen)
- ✓ Have each group's original Pilot Scoping worksheet ready
- ✓ Print decision templates (1 page per group)
- ✓ Have a timer visible (helps with pacing)

### During the Exercise
- ✓ Circulate while groups work
- ✓ Ask guiding questions (don't give answers)
- ✓ Make sure each group makes a clear decision (no wishy-washy)
- ✓ Note which groups chose which path (for debrief)

### During Presentations
- ✓ Listen for reasoning quality
- ✓ Ask follow-up questions
- ✓ Note where groups agree/disagree
- ✓ Use disagreements to teach decision frameworks

### During Debrief
- ✓ Let groups discover the logic
- ✓ Land the key points (data > gut, solvable vs. fundamental, pivot is normal)
- ✓ Connect back to earlier exercises
- ✓ Show that this is what real PM work looks like

---

