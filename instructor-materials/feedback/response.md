This feedback often doesn't mean "not enough content," it means "not enough density or advanced application." One participant even noted that discussion times were "way too long" , which confirms it's a pacing and depth issue, not a volume issue.

Your paper-based, group-discussion approach is excellent for teaching leadership, communication, and strategy—which your feedback confirms was the most successful part (e.g., "common failure reason... is the human component" ).

The problem is the expectation mismatch. Participants heard "AI" and expected a technical/practical class ("how to use/integrate AI" ), but you delivered a strategic/leadership class.

You can fix this by strategically "leveling up" your existing activities and integrating tech in a controlled way.

## How to Address: "Too Light" & "Not at PM Level"

This is about adding analytical rigor and strategic depth to your existing activities, not adding more activities.

Aggressively Set Expectations: Start the masterclass by being explicit. "This is a strategic leadership masterclass on managing AI projects, focusing on the human, ethical, and change management challenges. This is not a technical, hands-on coding or platform workshop." This filters audience expectations immediately.

Introduce "Level 2" Frameworks: The attendees wanting an "executive" or "level 2" experience are bored by standard PM exercises. Elevate the existing activities:

Go/No-Go Activity: Don't just have them identify materials. Give them a formal, weighted decision-matrix (e.g., a scorecard with criteria like Strategic Fit, Technical Feasibility, Data Readiness, Ethical Risk). Have teams debate the weights for the criteria first, then score the project. This is a classic "executive" activity that turns a simple discussion into a rigorous, justifiable decision.

Crisis Management Activity: Make the scenarios more complex. Instead of just "data quality issues" , provide a (fake) snippet of the biased data and ask them to identify the type of bias (e.g., sampling, measurement, historical) and link it to a specific, disastrous business outcome.

Tighten the Pacing: You have a 6-hour day. Use a visible timer for discussions. As Karl noted, "allocated discussion times were way too long" . Tighter deadlines (e.g., "You have 15 minutes for this, go!") create urgency, focus the discussion, and make the day feel denser and more energetic.

How to Address: "Didn't 'Actually Use' AI" & "More Real Examples"
You can integrate both without sacrificing your group-discussion model. Your instinct about laptops turning it into an individual experience is correct, so don't give everyone a laptop.

The "One Tool per Table" Strategy: Keep the paper-based activities, but add one tablet or laptop per table for specific, time-boxed tasks.

New Activity Idea: During your "Project Scoping" or "Stakeholder Management" module, add a 20-minute task. "Using the AI tool at your table (e.g., ChatGPT/Claude), as a group, you must craft three prompts to help you identify project risks for RetailFlow."

This "group-prompting" exercise achieves two goals:

They "actually use AI" .

It's still a group discussion, now focused on how to ask the right questions—a key leadership skill. They can then discuss the (often flawed) AI-generated output, which is a great lesson in itself.

The "10-Minute Mini-Case" Opener: To address the "more real examples" feedback, start each of your four modules with a different, 10-minute, real-world mini-case.

Module 1 (Scoping): Start with a 1-page summary of a successful AI project (e.g., how a real company used AI to improve customer service). Ask: "What was their 'go/no-go' criteria?"

Module 3 (Crisis): Start with a 1-page summary of a failed AI project (e.g., Amazon's biased recruiting tool, Microsoft's Tay chatbot). Ask: "Which of our four crises (data, team, scope, ethics) was the primary cause of failure?"

This directly injects the real-world examples they craved and provides immediate, concrete context for your RetailFlow activities.
