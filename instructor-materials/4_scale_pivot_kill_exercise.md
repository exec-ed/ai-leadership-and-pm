# SCALE/PIVOT/KILL DECISION EXERCISE
## Staff Answers & Facilitation Guide

**Activity Duration:** 45 min (exercise + brief debrief)
**Format:** Groups analyze pilot data and make a go/no-go decision
**Difficulty:** Hard
**Key Skill Being Tested:** Decision-making with data, Go/No-Go criteria application, business judgment

---

## ACTIVITY OVERVIEW

After the Crisis Simulation, participants receive data from their completed pilot. Using the Go/No-Go criteria they defined in Pilot Scoping, they must decide: **Scale the project, Pivot to a new approach, or Kill it.**

This exercise forces them to apply their own criteria and make a hard call with ambiguous data.

---

## SCENARIO SETUP

**Time in scenario:** End of Month 6 pilot. Decision point.

**What participants receive:**
- Pilot performance data (30 sheets with metrics, dashboards, qualitative feedback)
- Original scoping document (their own criteria from Exercise 1)
- Go/No-Go criteria they defined
- 45 minutes to analyze and decide

**What they must do:**
- Analyze the data against their own criteria
- Decide: Scale, Pivot, or Kill
- Write a brief recommendation (1 page)
- Present their decision to the room (3 min)

---

## SAMPLE PILOT DATA (What You Give Them)

You can adapt this based on your actual masterclass, but here's what good data looks like:

### KEY METRICS (After 6 weeks of pilot)

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| **Accuracy** | ≥90% | 88% | ⚠️ Just below target |
| **Customer Satisfaction** | ≥80% | 82% | ✓ Exceeded |
| **Response Time** | <4 hours | 3.5 hours | ✓ Exceeded |
| **Cost per Query** | $12 (was $18) | $14 | ⚠️ Slightly below target |
| **Team Adoption** | 80%+ using AI | 75% using AI | ⚠️ Slight resistance |
| **Escalations Due to AI Error** | <2% | 3.5% | ❌ Above target |

---

### QUALITATIVE FEEDBACK

**From customers (satisfaction survey):**
- 82% satisfied overall
- "Fast and accurate most of the time"
- "Sometimes got wrong information, but could escalate to human"
- "Prefer this to 26-hour email wait time"

**From customer service team:**
- "AI handles the boring stuff. Frees us up for real problems."
- "Some accuracy issues, but mostly good."
- "Still worried about future" (2 agents)
- "Would use it more if accuracy improved"

**From leadership:**
- CEO: "Good progress. Still want to see higher accuracy before we scale."
- CFO: "Cost savings not as high as projected. Need to recalibrate expectations."
- Data Scientist: "Can improve accuracy to 92% in 4 weeks with additional training."

---

### INCIDENT LOG

- Week 1: Data quality issue (fixed)
- Week 2: Team resistance (addressed)
- Week 3: Escalations spiked (returned to baseline)
- Week 4-6: Steady performance

---

## STAFF ANSWER - Model Solution

### Option 1: SCALE ❌ (Not Recommended Based on Data)

**Why this would be wrong:**
- Accuracy: 88% vs. target 90% (below target)
- Escalations: 3.5% vs. target <2% (significantly above)
- Team adoption: 75% vs. target 80% (below target)
- Cost savings: $14/query vs. target $12 (below target)

**Staff verdict:** You only meet 2 of 6 metrics. That's not "scale." You haven't proved competence.

---

### Option 2: PIVOT ✓ (RECOMMENDED)

**The recommendation:**

> "PIVOT: Extend pilot 4 more weeks with focus on accuracy improvements.
>
> **Rationale:**
> - We're close on accuracy (88% vs. 90%). Data scientist says 4 weeks to get to 92%.
> - We exceeded on satisfaction (82%) and response time (3.5 hours). That's proof of concept.
> - Cost savings lagging but understandable at small scale; volume increases will improve this.
> - Team adoption at 75% is actually solid for new technology. Two holdouts don't veto the project.
> - Escalations spiked early then normalized. System is learning.
>
> **Next Steps:**
> - Invest 4 more weeks in model improvement (accuracy focus)
> - Add return policy queries (Phase 2) to increase volume and improve ROI
> - Then reassess at Week 10 for scale decision
> - This gives us cleaner data and higher confidence"

**Why PIVOT is correct:**
- You're not at success yet (data shows it)
- But you're on the right track (satisfaction, response time, team mood)
- The issues are solvable (accuracy, escalations, cost)
- Extending 4 weeks is low-risk way to prove concept
- You show leadership you're data-driven ("I wanted to scale but data doesn't support it yet")

---

### Option 3: KILL ❌ (Also Not Recommended)

**Why this would be wrong:**
- You HAVE proof of value (82% satisfaction, 3.5 hr response time)
- You HAVE team adoption (75% is solid)
- You DON'T have evidence that the system fundamentally can't work
- You're only 2 percentage points away on accuracy

**Staff verdict:** Kill would be overly cautious. You have enough signal to continue. Just not enough to scale yet.

---

## DECISION FRAMEWORK (How to Guide Them)

If participants are struggling, ask these questions:

1. **"How many of your success criteria did you meet?"**
   - If >80% of criteria met → Consider Scale
   - If 50-80% met → Consider Pivot
   - If <50% met → Consider Kill

2. **"Are the gaps due to fundamental problems or solvable issues?"**
   - Fundamental (data quality is garbage, team won't use it) → Kill
   - Solvable (accuracy needs 4 more weeks, escalations are normalizing) → Pivot
   - Not present (metrics are good) → Scale

3. **"What's your biggest assumption? Is it being tested?"**
   - "Assumption: AI can work in customer service" → Tested in pilot, VALIDATED
   - "Assumption: Accuracy will hit 90%" → Tested, NOT YET (but close)
   - If key assumption is proven wrong → Kill

4. **"How would you explain this decision to the board?"**
   - If you can explain it clearly with data → Probably the right call
   - If you're nervous about the explanation → Reconsider

---

## COMMON MISTAKES PARTICIPANTS MAKE

### Mistake #1: "We Spent $150K So We Have to Scale"
**Sunk Cost Fallacy.** Money already spent doesn't matter. Only future return matters.

**Correction:** "The $150K is sunk. Question is: Will scaling this system create more value than other options? If yes, scale. If no, kill and redirect the budget."

---

### Mistake #2: "It's Almost There, So We Scale"
**Being too optimistic.** "Almost" isn't "there yet."

**Correction:** "Accuracy is 88% vs. 90% target. That's a 2-point gap, but it's a meaningful gap. Would you drive a car with 88% brake reliability? No. Same principle. Pivot, get to 90%, then scale with confidence."

---

### Mistake #3: "We Can't Kill This, Look at Satisfaction Levels"
**Cherry-picking positive data.** Satisfaction is good, but other metrics matter too.

**Correction:** "Satisfaction IS good (82%). But escalations are 3.5% vs. target <2%. Cost is $14 vs. target $12. Both matter. You can't scale on good satisfaction alone."

---

### Mistake #4: "We Should Scale and Fix Issues in Production"
**Too risky, especially in post-Crisis #1.** That's how you scale a bad system to all customers.

**Correction:** "Remember Crisis #1? That's what happens when you scale fast with unresolved issues. You have 4 weeks and a clear path to accuracy. Better to use it."

---

## WHAT "GOOD" LOOKS LIKE

### Strong Decision-Making (9-10)
- ✓ References their own Go/No-Go criteria
- ✓ Analyzes data against criteria (not cherry-picking)
- ✓ Identifies which gaps are solvable vs. fundamental
- ✓ Makes a clear call: Scale OR Pivot OR Kill (not wishy-washy)
- ✓ Explains the decision with data
- ✓ Shows they understand trade-offs

**Example:**
> "We miss accuracy target (88% vs. 90%) and escalation target (3.5% vs. <2%). But we hit satisfaction (82%), response time (3.5 hr), and team adoption (75%). Data scientist says 4 weeks to accuracy. PIVOT: Extend 4 weeks, improve accuracy, then scale. This is data-driven and confident."

---

### Competent Decision-Making (7-8)
- ~ Uses criteria but doesn't reference explicitly
- ~ Analyzes most data points
- ~ Makes a decision but reasoning could be clearer
- ~ Shows understanding of some trade-offs

**Example:**
> "The metrics are mixed. Some good, some not great. We should probably pivot and try again. 4 more weeks seems reasonable."

---

### Developing Decision-Making (5-6)
- ~ References criteria vaguely
- ~ Analyzes only some data
- ~ Decision is unclear or conflicts with data
- ~ Limited trade-off thinking

**Example:**
> "We spent a lot of money. Customers are mostly happy. We should probably scale but maybe fix some things first."

---

### Needs Development (<5)
- ❌ No reference to criteria
- ❌ Cherry-picks data
- ❌ Decision doesn't follow the data
- ❌ No clear reasoning

**Example:**
> "I think this is good. We should scale it."

---

## DEBRIEF (5-10 min after presentations)

### If Time Allows, Quick Debrief Questions

**"Which groups chose Pivot and which chose Scale?"**
- Likely most choose Pivot (the correct answer given the data)
- If some chose Scale: "What data supported that?" (Usually they're cherry-picking)
- If some chose Kill: "What would change your mind?" (Testing their criteria)

---

**"Did your original criteria help you make this decision?"**
- If yes: "Good. That's why you define criteria upfront."
- If no: "Why not? Were your criteria not specific enough? Too ambitious? This tells you something about your planning."

---

**"How confident are you in your decision?"**
- High confidence → Good decision-making
- Low confidence → Either decision is truly ambiguous OR analysis could be better

---

## KEY LEARNING POINTS

### 1. "Data Beats Gut Feel"
You have actual numbers. Use them. Don't pretend that satisfaction alone justifies scaling when accuracy is below target.

### 2. "Go/No-Go Criteria Are Your Anchor"
If you defined them upfront, this decision is easier. "We said accuracy ≥90%. We're at 88%. We don't scale." Done.

### 3. "Pivot Is Often the Right Answer"
Not everything is a binary Scale vs. Kill. Pivot (continue with changes) is often the most data-driven choice.

### 4. "Sunk Costs Don't Justify Future Spending"
You spent $150K. That's done. The question is: Will spending more money on scale yield more return? Justify it by forward returns, not backward costs.

### 5. "You Can Make a Decision Even with Ambiguous Data"
Perfect data doesn't exist. You make the best call with what you have. That's PM work.

---

## FACILITATOR TIPS

### During Exercise
- **Don't help them too much:** Let them struggle with the data
- **Ask clarifying questions:** "Why did you choose Scale?" Listen to their reasoning
- **Inject pressure:** "You have 10 minutes left. Make a decision."

### During Presentations
- **Listen to the reasoning:** How they got to their decision matters as much as the decision itself
- **Ask follow-ups:** "If accuracy was at 90%, would you scale?" (Tests whether they understand criteria)
- **Notice disagreements within groups:** "You two disagreed. What was the debate?"

### During Debrief
- **Don't announce the "right answer":** Let them discuss
- **Ask them to defend their data interpretation:** "Why do you think accuracy is acceptable?"
- **Land the learning:** "Notice what you just did: You made a hard decision with limited data. That's PM work. This is what it feels like."

---

## VARIATIONS (Depending on Time/Preferences)

### Variation 1: Assigned Different Decisions
- Half the groups get "Scale scenario" (strong data)
- Half get "Kill scenario" (weak data)
- They debate which groups' decisions are sound

### Variation 2: Add Executive Pushback
- Facilitator plays CEO
- CEO disagrees with Kill decision
- PM has to defend their reasoning

### Variation 3: Add Cost Constraint
- "You can extend the pilot OR pivot with half the budget. Choose."
- Forces trade-off thinking

---

## CONNECTION TO EARLIER ACTIVITIES

This exercise ties back to everything:

**Pilot Scoping** → You defined Go/No-Go criteria here
- "If accuracy <85%, we kill"
- "If satisfaction ≥80%, we scale"

**Crisis Simulation** → You learned that issues are solvable
- Crisis #1 taught you: Data problems are fixable (Pivot)
- Crisis #2 taught you: Team resistance is solvable (Pivot)

**Speed-Dating** → You know stakeholder concerns matter
- CEO wants ROI (look at cost data)
- Data Scientist wants accuracy (look at 88% vs. 90%)
- Manager wants team adoption (look at 75%)

**This exercise** → Now you apply all of it to a real decision

---

**Next Activity:** Personal Action Planning (how they'll apply this to their real projects)
