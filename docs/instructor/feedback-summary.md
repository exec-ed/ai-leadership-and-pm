# ðŸ“ˆ Overall Summary

Overall feedback for the masterclass was positive, but mixed.

Participants gave very high ratings for engagement (Avg: 4.60) and practicality (Avg: 4.21), with many praising the case study-based approach and the focus on the "human component" of AI projects (like change management and stakeholder engagement) rather than just the technology.

However, the question of whether the masterclass "met expectations" received the lowest and most varied scores (Avg: 3.93). This was driven by a few participants who felt the content was too basic and not at an "executive" level, or who were expecting a more technical, hands-on deep-dive into specific AI platforms rather than a project management and leadership perspective.

## ðŸ“Š Quantitative Feedback (Questions 1-4)

Here is a table containing the key statistics you requested (Min, Max, Average) and the quartile data needed to build the plots. The scores are rated on a scale of 1 (low) to 5 (high).

Question	Min	Q1 (25th Percentile)	Median (Q2)	Q3 (75th Percentile)	Max	Average	Count (n)
1. Met my expectations.	1	3.0	4.0	5.0	5	3.93	15
2. Included practical strategies.	3	4.0	4.0	5.0	5	4.21	14
3. Was engaging and interesting.	4	4.0	5.0	5.0	5	4.60	15
4. Would recommend to others.	4	4.0	4.5	5.0	5	4.50	14


## ðŸ“ Qualitative Feedback Summary

### Suggestions for Improvement

Content Level & Focus: This was the most common critique. Some participants wanted "â€˜level 2â€™ content" and felt the material was "very light" and "not at project management level" . One attendee was "expecting a more practical approach on how to use/integrate AI into business applications, not the project management part of it" .

**Case Studies & Examples:** While many liked the case study, suggestions included adding "more real-world case studies" , "more real examples" , and "more in-depth explanation for the case study answers" .

**Pacing & Structure:** One participant felt the "allocated discussion times were way too long" , while another suggested the case study content was "a bit too much" and could be improved with "directive questions for the reading" .

**Positive:** Several attendees stated "None" or "No" for improvements, with one calling it "one of the more engaging masterclasses I have attended" .

### Testimonials

Positive testimonials highlighted the masterclass's practical and strategic value.

*"This masterclass fundamentally shifted my perspective on leadership. The practical frameworks for applying AI have already made our team more proactive and data-driven..."*

*"It is inspiring and practical. It showed how AI can enhance strategic decision-making, improve team efficiency, and drive innovation in real-world projects."*

*"This masterclass is useful, especially it is taught with a case study."*

## Future Topics Requested
- AI tools for project risk analysis
- AI in financial modeling
- Sustainable development

## ðŸŽ“ Credit Question Summaries

1. Main Theories & Concepts Explored

Participants identified several key themes from the masterclass:

- AI Project Lifecycle: The most frequently mentioned concept was the AI project lifecycle: "ideation -> scoping -> pilot -> evaluation -> scale, pivot or kill" .

- The "Human Component": A major takeaway was that AI project failures are often due to "the human component (rather than technical issue)" . This included the need for "change management" , handling "team resistance" , and managing stakeholder communication.

- Data Governance: Many noted the importance of data, stating "AI requires clean data, otherwise it will create biased and faulty decisions" and the need to "handle data quality issues" .

- Strategic & Leadership Shift: The class explored "how AI supports leadership... through data-driven decision-making, adaptive leadership, and ethical AI use" and how the PM's role shifts from "administrative control to strategic leadership" .

- Practical PM Challenges: Attendees learned about managing "scope creep," "how to handle executive requests," and the "importance of setting realistic expectations" .

2. Workplace Application
Participants planned to apply the concepts in these key areas:

- Improved Project Management: Using AI "to enhance project planning and risk forecasting" and "managing changes carefully, setting clear priorities" .

- Leading Change & Stakeholders: Focusing on the "people" aspect. One participant, a People and Culture advisor, noted the key takeaway was "how can I engage my stakeholders... to ensure the success of the project" .

- Ethical Oversight: Applying "transparency, fairness, and agility" and ensuring they "control the process and not only focus on the end results created by AI" .

- Data-Driven Decisions: Moving "beyond intuition-based strategies to predictive analytics that forecast market trends and identify risks" .


This feedback often doesn't mean "not enough content," it means "not enough density or advanced application." One participant even noted that discussion times were "way too long" , which confirms it's a pacing and depth issue, not a volume issue.

Your paper-based, group-discussion approach is excellent for teaching leadership, communication, and strategyâ€”which your feedback confirms was the most successful part (e.g., "common failure reason... is the human component" ).

The problem is the expectation mismatch. Participants heard "AI" and expected a technical/practical class ("how to use/integrate AI" ), but you delivered a strategic/leadership class.

You can fix this by strategically "leveling up" your existing activities and integrating tech in a controlled way.

How to Address: "Too Light" & "Not at PM Level"
This is about adding analytical rigor and strategic depth to your existing activities, not adding more activities.

Aggressively Set Expectations: Start the masterclass by being explicit. "This is a strategic leadership masterclass on managing AI projects, focusing on the human, ethical, and change management challenges. This is not a technical, hands-on coding or platform workshop." This filters audience expectations immediately.

Introduce "Level 2" Frameworks: The attendees wanting an "executive" or "level 2" experience are bored by standard PM exercises. Elevate the existing activities:

Go/No-Go Activity: Don't just have them identify materials. Give them a formal, weighted decision-matrix (e.g., a scorecard with criteria like Strategic Fit, Technical Feasibility, Data Readiness, Ethical Risk). Have teams debate the weights for the criteria first, then score the project. This is a classic "executive" activity that turns a simple discussion into a rigorous, justifiable decision.

Crisis Management Activity: Make the scenarios more complex. Instead of just "data quality issues" , provide a (fake) snippet of the biased data and ask them to identify the type of bias (e.g., sampling, measurement, historical) and link it to a specific, disastrous business outcome.

Tighten the Pacing: You have a 6-hour day. Use a visible timer for discussions. As Karl noted, "allocated discussion times were way too long" . Tighter deadlines (e.g., "You have 15 minutes for this, go!") create urgency, focus the discussion, and make the day feel denser and more energetic.

How to Address: "Didn't 'Actually Use' AI" & "More Real Examples"
You can integrate both without sacrificing your group-discussion model. Your instinct about laptops turning it into an individual experience is correct, so don't give everyone a laptop.

The "One Tool per Table" Strategy: Keep the paper-based activities, but add one tablet or laptop per table for specific, time-boxed tasks.

New Activity Idea: During your "Project Scoping" or "Stakeholder Management" module, add a 20-minute task. "Using the AI tool at your table (e.g., ChatGPT/Claude), as a group, you must craft three prompts to help you identify project risks for RetailFlow."

This "group-prompting" exercise achieves two goals:

They "actually use AI" .

It's still a group discussion, now focused on how to ask the right questionsâ€”a key leadership skill. They can then discuss the (often flawed) AI-generated output, which is a great lesson in itself.

The "10-Minute Mini-Case" Opener: To address the "more real examples" feedback, start each of your four modules with a different, 10-minute, real-world mini-case.

Module 1 (Scoping): Start with a 1-page summary of a successful AI project (e.g., how a real company used AI to improve customer service). Ask: "What was their 'go/no-go' criteria?"

Module 3 (Crisis): Start with a 1-page summary of a failed AI project (e.g., Amazon's biased recruiting tool, Microsoft's Tay chatbot). Ask: "Which of our four crises (data, team, scope, ethics) was the primary cause of failure?"

This directly injects the real-world examples they craved and provides immediate, concrete context for your RetailFlow activities.

Your Final Question
No, simply providing additional material on the companion site is not enough.

That is a passive solution to an active learning problem. The feedback wasn't "I wish I had more to read later"; it was "My experience in the room felt too basic." The companion site is perfect for pre-reading (to level-set everyone) or post-reading (for those who want to go deeper), but it won't fix the "too light" or "didn't use AI" feedback. The changes need to happen during the six-hour masterclass.




Gemini can make mistakes, so double-check it


